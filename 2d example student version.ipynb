{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Practise exercise\n",
    "\n",
    "In this assignment we wish to use neural networks to estimate the two dimensional distribution of a dependent variable $Y = (Y_1,Y_2)$ as a function of covariates $x_1$ and $x_2$. This is *not* the hand-in exercise. Various questions and tasks are given throughout the notebook."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before you start, make sure that the relevant packages are installed (for instance by using pip install). All the necessary packages can be found in the requirements.txt file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pyreadr\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Input\n",
    "from tensorflow.keras.models import Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preliminaries\n",
    "\n",
    "We first load, split, and standardize the data. The blocks below load the NeuralNetworks.Rdata file. The first 400 points are used as training set and the final 100 as test set. We have also created standardized versions of covariates and targets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The blocks below load and split the data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = pyreadr.read_r('NeuralNetworks.Rdata')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = np.hstack((data['x1'], data['x2']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Y = np.hstack((data['Y1'], data['Y2']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = X[0:400]\n",
    "X_test = X[400:]\n",
    "Y_train = Y[0:400]\n",
    "Y_test = Y[400:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next block standardizes the covariates and targets.\n",
    "\n",
    "Question: Why do we use X_mean and X_std in line 6 and not X_test and X_std?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_mean = np.mean(X_train, axis=0)\n",
    "X_std = np.std(X_train, axis=0)\n",
    "Y_mean = np.mean(Y_train, axis=0)\n",
    "Y_std = np.std(Y_train, axis=0)\n",
    "\n",
    "X_train_n = (X_train - X_mean) / X_std\n",
    "X_test_n = (X_test - X_mean) / X_std\n",
    "\n",
    "Y_train_n = (Y_train - Y_mean) / Y_std\n",
    "Y_test_n = (Y_test - Y_mean) / Y_std"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question (a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fit a neural network with four (dense) layers: three layers with 30 nodes and one output layer with two nodes. Use a ReLu activation function for the first three layers, and a linear for the last one. Use the mean squared error \"mse\" as loss function. Determine the mean squared error on the test data for $Y_1$ and $Y_2$ separately."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model_dual_output():\n",
    "    \"\"\"Create and compile a simple neural network with two outputs\"\"\"\n",
    "    inputs = Input(shape=(2,))\n",
    "    x = Dense(30, activation='relu')(inputs)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    outputs = Dense(2, activation='linear')(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the validation_split option to find the optimal number of epochs. Train on the entire training set for the eventual model (set validation_split to 0.0). We want to find the point where the validation loss no longer decreases (why?)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = get_model_dual_output()  # Get the model\n",
    "model.fit(X_train_n, Y_train, epochs=..., batch_size=50, verbose=1, validation_split=0.2)  # Train the model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predictions_single_model = model.predict(X_test_n, verbose=0)  # Get the predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'MSE model with dual output: {(np.mean(np.square(predictions_single_model - Y_test), axis=0))}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question (b)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now fit a similar neural network, but only with one output, with $Y_1$ as target variable. Compare the mean squared error on the test data for $Y_1$ with the model from (a). Is there any difference? If so, can you give an explanation for the difference?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finish this code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_model_single_ouput():\n",
    "    \"\"\"Create and compile a simple network with a one-dimensional output\"\"\"\n",
    "    inputs = Input(shape=(2,))\n",
    "    x = Dense(30, activation='relu')(inputs)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    outputs = ...\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again, use the validation_split option to find the optimal training time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_2 = get_model_single_ouput()\n",
    "model_2.fit(X_train_n, Y_train[:, 0], epochs=..., batch_size=50, validation_split=0.0, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'MSE model single output: {}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question (c)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We want to see if the model performs better with standardized targets as well. Use Y_train_n (the standardized version). Retrain the models from questions (a) and (b). Make sure to find the new optimal training time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_3 = get_model_dual_output()\n",
    "model_3.fit(X_train_n, Y_train_n, validation_split=0.0, epochs=..., batch_size=50, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_4 = get_model_single_ouput()\n",
    "model_4.fit(X_train_n, Y_train_n[:, 0], validation_split=0.0, epochs=..., batch_size=50, verbose=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "How does the optimal training time differ? Can you explain this? What would happen if you used the same training time as for the model with the original targets?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Transform the predictions back to the original scale in order to compare the results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "residuals_model_3 = model_3.predict(X_test_n) * ... + ... - ... # transform back to original scale\n",
    "residuals_model_4 = model_4.predict(X_test_n) * ... + ... - ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print((np.mean(np.square(residuals_model_3), axis=0)))\n",
    "print((np.mean(np.square(residuals_model_4), axis=0)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question (d)\n",
    "\n",
    "\n",
    "We now have a predictor for (the standardized) $(Y_1,Y_2)$. We view this predictor as an estimator for the regression function $f$, where we model $Y = (Y_1,Y_2)$ for given covariates $x$ as $Y \\sim f(x)+ \\mathcal{N}(0, \\Sigma(x))$.\n",
    "Here, $f(x) \\in \\mathbb{R}^{2}$. We also wish to estimate the covariance matrix $\\Sigma(x)$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss function can be used to train a network that predicts a covariance  metric. As targets, the networks gets the residuals of the first network (that outputs predictions $\\hat{f}(x)$). This function outputs the negative loglikelihood of the data given a predicted covariance matrix. By using this as a loss function, we can find the optimal covariance matrix.\n",
    "\n",
    "Answer the following questions about this loss function:\n",
    "\n",
    "1. y_pred has three dimensions. The first two are the variances on logarithmic scale. These are then transformed to the normal scale in line 5. Why do we not output the variances directly?\n",
    "2. How do we determine the off-diagonal term of the covariance matrix? Hint: see lines 6 through 8. Why is this done in this way? Why do we not directly output $cov(Y_1, Y_2)$?\n",
    "3. In line 6, what is rho and why do we use this specific transformation?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def covariance_loss(y_true, y_pred):\n",
    "    residuals = y_true[:, 0:2]\n",
    "    residuals = tf.expand_dims(residuals, axis=-1)\n",
    "    log_variances = y_pred[:, 0:2]\n",
    "    variances = tf.math.exp(log_variances) + 1e-6\n",
    "    rho = 2 * (tf.math.sigmoid(y_pred[:, 2]) - 0.5)\n",
    "    det = variances[:, 0] * variances[:, 1] * (1 - tf.square(rho))\n",
    "    inv_cov_matrix =  tf.stack([\n",
    "                                tf.stack([variances[:, 1] / det,\n",
    "                                          -rho * tf.sqrt(variances[:, 0] * variances[:, 1]) / det], axis=1),\n",
    "                                tf.stack([-rho * tf.sqrt(variances[:, 0] * variances[:, 1]) / det,\n",
    "                                          variances[:, 0] / det], axis=1)\n",
    "                               ], axis=1)\n",
    "    mahalanobis_dist = tf.transpose(residuals, perm=[0, 2, 1])  @ inv_cov_matrix @ residuals\n",
    "    log_likelihood = - 0.5 * tf.math.log(det)  - 0.5 * mahalanobis_dist\n",
    "    return - log_likelihood"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The model below will be used to obtain the covariance function. Finish the final layer. Be careful to select the correct number of outputs and the correct activation function. Note that in line 8, we have given our custom loss function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_covariance_model():\n",
    "    inputs = Input(shape=(2))\n",
    "    x = Dense(30, activation='relu')(inputs)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    x = Dense(30, activation='relu')(x)\n",
    "    outputs = Dense(..., activation=...)(x)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss=covariance_loss)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create and train the model. Find the optimal training time."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covariance_model = get_covariance_model()\n",
    "train_residuals = model_3.predict(X_train_n) - Y_train_n\n",
    "covariance_model.fit(X_train_n, train_residuals, epochs=.., batch_size=50, validation_split=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We must transform the output of the model to a covariance matrix. Complete the code blow. Look at the custom loss function to see what parametrizations are used."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"A simple sigmoid function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "covariance_predictions = covariance_model.predict(X_test_n)\n",
    "covariance_matrix = np.zeros((len(X_test_n), 2,2))\n",
    "covariance_matrix[:, 0, 0] = ...(covariance_predictions[:, 0]) + ...\n",
    "covariance_matrix[:, 1, 1] = ...(covariance_predictions[:, 1]) + ...\n",
    "rho = (sigmoid(covariance_predictions[:, 1]) - 0.5) * ...\n",
    "covariance_matrix[:, 0, 1] = covariance_matrix[:, 1, 0] = np.sqrt(covariance_matrix[:, 0, 0] * covariance_matrix[:, 1, 1]) * ...\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check the following matrices to see if everything works as expected."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(covariance_matrix[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Can you think of a way to evaluate the performance of this estimator on a test set?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Question (e)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have a model that estimates $f(x)$ and a model that estimates $\\Sigma(x)$. We can use these to simulate new targets. This allows us to repeat the entire procedure many times to see if this procedure produces sensible results. It is up to you to think of a way to quantify 'sensible results'. We first provide some code to simulate new targets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function below simulates a new set of targets given mean predictions and covariance matrix predictions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def gen_new_targets(means, covariance_matrices):\n",
    "    \"\"\"Simulate new targets\n",
    "\n",
    "    Arguments:\n",
    "        means: An array containing the predicted means\n",
    "        covariance_matrices: An array containing the predicted covariance matrices\n",
    "\n",
    "    Returns:\n",
    "        new_targets: An array containing simulated targets.\n",
    "    \"\"\"\n",
    "    simulated_targets = np.zeros_like(means)\n",
    "    for i, m in enumerate(means):\n",
    "        new_target = np.random.multivariate_normal(m, cov=covariance_matrices[i])\n",
    "        simulated_targets[i] = new_target\n",
    "    return simulated_targets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Get the mean predictions and covariance matrix predictions for the training set. Make sure to use the same model for $\\hat{f}(x)$ that was used to obtain the residuals that were used to obtain $\\hat{\\Sigma}(x)$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mean_predictions_train = model_3.predict(X_train_n, verbose=0)\n",
    "covariance_predictions_train = covariance_model.predict(X_train_n, verbose=0)\n",
    "covariance_matrices_train = np.zeros((len(X_train_n), 2,2))\n",
    "covariance_matrices_train[:, 0, 0] = ...\n",
    "covariance_matrices_train[:, 1, 1] = ...\n",
    "rho = ...\n",
    "covariance_matrices_train[:, 0, 1] = covariance_matrices_train[:, 1, 0] = ..."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "new_targets = gen_new_targets(mean_predictions_train, covariance_matrices_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot a new set of simulated targets and compare with the original. Does it look reasonable?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('Simulated targets (normalized scale)')\n",
    "plt.scatter(X_train[:, 1], new_targets[:, 0])\n",
    "plt.scatter(X_train[:, 1], new_targets[:, 1])\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title('Original targets (normalized scale)')\n",
    "plt.scatter(X_train[:, 1], Y_train_n[:, 0])\n",
    "plt.scatter(X_train[:, 1], Y_train_n[:, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To do: construct and run the simulation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "N_simulations = 100\n",
    "for i in range(N_simulations):\n",
    "    # Simulate new targets\n",
    "    # Train models\n",
    "    # Evaluate models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}